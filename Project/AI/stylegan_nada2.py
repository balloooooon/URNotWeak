# -*- coding: utf-8 -*-
"""stylegan_nada2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WVMU8T6X1a2EWom72MN_GJJatU5fipZV

# Welcome to StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators!

# Step 1: Setup required libraries and models.
This may take a few minutes.

You may optionally enable downloads with pydrive in order to authenticate and avoid drive download limits when fetching pre-trained ReStyle and StyleGAN2 models.
"""

# Commented out IPython magic to ensure Python compatibility.
#@title Setup

import os

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

pretrained_model_dir = os.path.join("/content", "models")
os.makedirs(pretrained_model_dir, exist_ok=True)

restyle_dir = os.path.join("/content", "restyle")
stylegan_ada_dir = os.path.join("/content", "stylegan_ada")
stylegan_nada_dir = os.path.join("/content", "stylegan_nada")

output_dir = os.path.join("/content", "output")

output_model_dir = os.path.join(output_dir, "models")
output_image_dir = os.path.join(output_dir, "images")

download_with_pydrive = True #@param {type:"boolean"}

class Downloader(object):
    def __init__(self, use_pydrive):
        self.use_pydrive = use_pydrive

        if self.use_pydrive:
            self.authenticate()

    def authenticate(self):
        auth.authenticate_user()
        gauth = GoogleAuth()
        gauth.credentials = GoogleCredentials.get_application_default()
        self.drive = GoogleDrive(gauth)

    def download_file(self, file_id, file_dst):
        if self.use_pydrive:
            downloaded = self.drive.CreateFile({'id':file_id})
            downloaded.FetchMetadata(fetch_all=True)
            downloaded.GetContentFile(file_dst)
        else:
            !gdown --id $file_id -O $file_dst

downloader = Downloader(download_with_pydrive)

# # install requirements
# !git clone https://github.com/yuval-alaluf/restyle-encoder.git $restyle_dir

# !wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip
# !sudo unzip ninja-linux.zip -d /usr/local/bin/
# !sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force

# !pip install ftfy regex tqdm
# !pip install git+https://github.com/openai/CLIP.git

# !git clone https://github.com/NVlabs/stylegan2-ada/ $stylegan_ada_dir
# !git clone https://github.com/rinongal/stylegan-nada.git $stylegan_nada_dir

from argparse import Namespace

import sys
import numpy as np

from PIL import Image

import torch
import torchvision.transforms as transforms

sys.path.append(restyle_dir)
sys.path.append(stylegan_nada_dir)
sys.path.append(os.path.join(stylegan_nada_dir, "ZSSGAN"))

device = 'cuda'

# %load_ext autoreload
# %autoreload 2

"""# Step 2: Choose a model type.
Model will be downloaded and converted to a pytorch compatible version.

Re-runs of the cell with the same model will re-use the previously downloaded version. Feel free to experiment and come back to previous models :)
"""

source_model_type = 'ffhq' #@param['ffhq', 'cat', 'dog', 'church', 'horse', 'car']

source_model_download_path = {"ffhq":   "1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT",
                              "cat":    "https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqcat.pkl",
                              "dog":    "https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqdog.pkl",
                              "church": "1iDo5cUgbwsJEt2uwfgDy_iPlaT-lLZmi",
                              "car":    "1i-39ztut-VdUVUiFuUrwdsItR--HF81w",
                              "horse":  "1irwWI291DolZhnQeW-ZyNWqZBjlWyJUn"}

model_names = {"ffhq":   "ffhq.pt",
               "cat":    "afhqcat.pkl",
               "dog":    "afhqdog.pkl",
               "church": "stylegan2-church-config-f.pkl",
               "car":    "stylegan2-car-config-f.pkl",
               "horse":  "stylegan2-horse-config-f.pkl"}

download_string = source_model_download_path[source_model_type]
file_name = model_names[source_model_type]
pt_file_name = file_name.split(".")[0] + ".pt"

dataset_sizes = {
    "ffhq":   1024,
    "cat":    512,
    "dog":    512,
    "church": 256,
    "horse":  256,
    "car":    512,
}

if not os.path.isfile(os.path.join(pretrained_model_dir, file_name)):
    print("Downloading chosen model...")

    if download_string.endswith(".pkl"):
        !wget $download_string -O $pretrained_model_dir/$file_name
    else:
        downloader.download_file(download_string, os.path.join(pretrained_model_dir, file_name))

if not os.path.isfile(os.path.join(pretrained_model_dir, pt_file_name)):
    print("Converting sg2 model. This may take a few minutes...")

    tf_path = next(filter(lambda x: "tensorflow" in x, sys.path), None)
    py_path = tf_path + f":{stylegan_nada_dir}/ZSSGAN"
    convert_script = os.path.join(stylegan_nada_dir, "convert_weight.py")
    # !PYTHONPATH=$py_path python $convert_script --repo $stylegan_ada_dir --gen $pretrained_model_dir/$file_name

"""# Step 3: Train the model.
Describe your source and target class. These describe the direction of change you're trying to apply (e.g. "photo" to "sketch", "dog" to "the joker" or "dog" to "avocado dog").

Alternatively, upload a directory with a small (~3) set of target style images (there is no need to preprocess them in any way) and set `style_image_dir` to point at them. This will use the images as a target rather than the source/class texts.

We reccomend leaving the 'improve shape' button unticked at first, as it will lead to an increase in running times and is often not needed.
For more drastic changes, turn it on and increase the number of iterations.

As a rule of thumb:
- Style and minor domain changes ('photo' -> 'sketch') require ~200-400 iterations.
- Identity changes ('person' -> 'taylor swift') require ~150-200 iterations.
- Simple in-domain changes ('face' -> 'smiling face') may require as few as 50.
- The `style_image_dir` option often requires ~400-600 iterations.

> Updates: <br>
> 03/10 - Added support for style image targets. <br>
> 03/08 - Added support for saving model checkpoints. If you want to save, set save_interval > 0.
"""

from ZSSGAN.model.ZSSGAN import ZSSGAN

import numpy as np

import torch

from tqdm import notebook

from ZSSGAN.utils.file_utils import save_images, get_dir_img_list
from ZSSGAN.utils.training_utils import mixing_noise

from IPython.display import display

source_class = "Photo" #@param {"type": "string"}
target_class = "Zombie" #@param {"type": "string"}

style_image_dir = "" #@param {'type': 'string'}

target_img_list = get_dir_img_list(style_image_dir) if style_image_dir else None

improve_shape = False #@param{type:"boolean"}

model_choice = ["ViT-B/32", "ViT-B/16"]
model_weights = [1.0, 0.0]

if improve_shape or style_image_dir:
    model_weights[1] = 1.0

mixing = 0.9 if improve_shape else 0.0

auto_layers_k = int(2 * (2 * np.log2(dataset_sizes[source_model_type]) - 2) / 3) if improve_shape else 0
auto_layer_iters = 1 if improve_shape else 0

training_iterations = 51 #@param {type: "integer"}
output_interval     = 50 #@param {type: "integer"}
save_interval       = 0 #@param {type: "integer"}

training_args = {
    "size": dataset_sizes[source_model_type],
    "batch": 2,
    "n_sample": 4,
    "output_dir": output_dir,
    "lr": 0.002,
    "frozen_gen_ckpt": os.path.join(pretrained_model_dir, pt_file_name),
    "train_gen_ckpt": os.path.join(pretrained_model_dir, pt_file_name),
    "iter": training_iterations,
    "source_class": source_class,
    "target_class": target_class,
    "lambda_direction": 1.0,
    "lambda_patch": 0.0,
    "lambda_global": 0.0,
    "lambda_texture": 0.0,
    "lambda_manifold": 0.0,
    "auto_layer_k": auto_layers_k,
    "auto_layer_iters": auto_layer_iters,
    "auto_layer_batch": 8,
    "output_interval": 50,
    "clip_models": model_choice,
    "clip_model_weights": model_weights,
    "mixing": mixing,
    "phase": None,
    "sample_truncation": 0.7,
    "save_interval": save_interval,
    "target_img_list": target_img_list,
    "img2img_batch": 16,
    "channel_multiplier": 2,
    "sg3": False,
    "sgxl": False,
}

args = Namespace(**training_args)

print("Loading base models...")
net = ZSSGAN(args)
print("Models loaded! Starting training...")

g_reg_ratio = 4 / 5

g_optim = torch.optim.Adam(
    net.generator_trainable.parameters(),
    lr=args.lr * g_reg_ratio,
    betas=(0 ** g_reg_ratio, 0.99 ** g_reg_ratio),
)

# Set up output directories.
sample_dir = os.path.join(args.output_dir, "sample")
ckpt_dir   = os.path.join(args.output_dir, "checkpoint")

os.makedirs(sample_dir, exist_ok=True)
os.makedirs(ckpt_dir, exist_ok=True)

seed = 3 #@param {"type": "integer"}

torch.manual_seed(seed)
np.random.seed(seed)

# Training loop
fixed_z = torch.randn(args.n_sample, 512, device=device)

for i in notebook.tqdm(range(args.iter)):
    net.train()

    sample_z = mixing_noise(args.batch, 512, args.mixing, device)

    [sampled_src, sampled_dst], clip_loss = net(sample_z)

    net.zero_grad()
    clip_loss.backward()

    g_optim.step()

    if i % output_interval == 0:
        net.eval()

        with torch.no_grad():
            [sampled_src, sampled_dst], loss = net([fixed_z], truncation=args.sample_truncation)

            if source_model_type == 'car':
                sampled_dst = sampled_dst[:, :, 64:448, :]

            grid_rows = 4

            save_images(sampled_dst, sample_dir, "dst", grid_rows, i)

            img = Image.open(os.path.join(sample_dir, f"dst_{str(i).zfill(6)}.jpg")).resize((1024, 256))
            display(img)

    if (args.save_interval > 0) and (i > 0) and (i % args.save_interval == 0):
        torch.save(
            {
                "g_ema": net.generator_trainable.generator.state_dict(),
                "g_optim": g_optim.state_dict(),
            },
            f"{ckpt_dir}/{str(i).zfill(6)}.pt",
        )

"""# Step 4: Generate samples with the new model"""

truncation = 0.7 #@param {type:"slider", min:0, max:1, step:0.05}

samples = 9

with torch.no_grad():
    net.eval()
    sample_z = torch.randn(samples, 512, device=device)

    [sampled_src, sampled_dst], loss = net([sample_z], truncation=truncation)

    if source_model_type == 'car':
        sampled_dst = sampled_dst[:, :, 64:448, :]

    grid_rows = int(samples ** 0.5)

    save_images(sampled_dst, sample_dir, "sampled", grid_rows, 0)

    display(Image.open(os.path.join(sample_dir, f"sampled_{str(0).zfill(6)}.jpg")).resize((768, 768)))

"""## Editing a real image with Re-Style inversion (currently only FFHQ inversion is supported):

Step 1: Set up Re-Style.

This may take a few minutes
"""

from restyle.utils.common import tensor2im
from restyle.models.psp import pSp
from restyle.models.e4e import e4e

downloader.download_file("1sw6I2lRIB0MpuJkpc8F5BJiSZrc0hjfE", os.path.join(pretrained_model_dir, "restyle_psp_ffhq_encode.pt"))
downloader.download_file("1e2oXVeBPXMQoUoC_4TNwAWpOPpSEhE_e", os.path.join(pretrained_model_dir, "restyle_e4e_ffhq_encode.pt"))

"""Step 2: Choose a re-style model

We reccomend choosing the e4e model as it performs better under domain translations. Choose pSp for better reconstructions on minor domain changes (typically those that require less than 150 training steps).
"""

encoder_type = 'e4e' #@param['psp', 'e4e']

restyle_experiment_args = {
    "model_path": os.path.join(pretrained_model_dir, f"restyle_{encoder_type}_ffhq_encode.pt"),
    "transform": transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])
}

model_path = restyle_experiment_args['model_path']
ckpt = torch.load(model_path, map_location='cpu')

opts = ckpt['opts']

opts['checkpoint_path'] = model_path
opts = Namespace(**opts)

restyle_net = (pSp if encoder_type == 'psp' else e4e)(opts)

restyle_net.eval()
restyle_net.cuda()
print('Model successfully loaded!')

def run_alignment(image_path):
    import dlib
    from scripts.align_faces_parallel import align_face
    if not os.path.exists("shape_predictor_68_face_landmarks.dat"):
        print('Downloading files for aligning face image...')
        os.system('wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2')
        os.system('bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2')
        print('Done.')
    predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
    aligned_image = align_face(filepath=image_path, predictor=predictor)
    print("Aligned image has shape: {}".format(aligned_image.size))
    return aligned_image

def get_avg_image(net):
    avg_image = net(net.latent_avg.unsqueeze(0),
                    input_code=True,
                    randomize_noise=False,
                    return_latents=False,
                    average_code=True)[0]
    avg_image = avg_image.to('cuda').float().detach()
    return avg_image

opts.n_iters_per_batch = 5
opts.resize_outputs = False  # generate outputs at full resolution

from restyle.utils.inference_utils import run_on_batch

"""Step 3: Align and invert an image"""

def run_alignment(image_path):
    import dlib
    from scripts.align_faces_parallel import align_face
    if not os.path.exists("shape_predictor_68_face_landmarks.dat"):
        print('Downloading files for aligning face image...')
        os.system('wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2')
        os.system('bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2')
        print('Done.')
    predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
    aligned_image = align_face(filepath=image_path, predictor=predictor)
    print("Aligned image has shape: {}".format(aligned_image.size))
    return aligned_image

image_path = "/content/suzy.png" #@param {'type': 'string'}
original_image = Image.open(image_path).convert("RGB")

input_image = run_alignment(image_path)

display(input_image)

img_transforms = restyle_experiment_args['transform']
transformed_image = img_transforms(input_image)

def get_avg_image(net):
    avg_image = net(net.latent_avg.unsqueeze(0),
                    input_code=True,
                    randomize_noise=False,
                    return_latents=False,
                    average_code=True)[0]
    avg_image = avg_image.to('cuda').float().detach()
    return avg_image

opts.n_iters_per_batch = 5
opts.resize_outputs = False  # generate outputs at full resolution

from restyle.utils.inference_utils import run_on_batch

with torch.no_grad():
    avg_image = get_avg_image(restyle_net)
    result_batch, result_latents = run_on_batch(transformed_image.unsqueeze(0).cuda(), restyle_net, opts, avg_image)

"""Step 4: Convert the image to the new domain"""

#@title Convert inverted image.
inverted_latent = torch.Tensor(result_latents[0][4]).cuda().unsqueeze(0).unsqueeze(1)

with torch.no_grad():
    net.eval()

    [sampled_src, sampled_dst] = net(inverted_latent, input_is_latent=True)[0]

    joined_img = torch.cat([sampled_src, sampled_dst], dim=0)
    save_images(joined_img, sample_dir, "joined", 2, 0)
    display(Image.open(os.path.join(sample_dir, f"joined_{str(0).zfill(6)}.jpg")).resize((512, 256)))

def predict(image_path):
  original_image = Image.open(image_path).convert("RGB")

  input_image = run_alignment(image_path)

  display(input_image)

  img_transforms = restyle_experiment_args['transform']
  transformed_image = img_transforms(input_image)

  with torch.no_grad():
    avg_image = get_avg_image(restyle_net)
    result_batch, result_latents = run_on_batch(transformed_image.unsqueeze(0).cuda(), restyle_net, opts, avg_image)


  #@title Convert inverted image.
  inverted_latent = torch.Tensor(result_latents[0][4]).cuda().unsqueeze(0).unsqueeze(1)

  with torch.no_grad():
      net.eval()

      [sampled_src, sampled_dst] = net(inverted_latent, input_is_latent=True)[0]

      joined_img = torch.cat([sampled_src, sampled_dst], dim=0)
      save_images(joined_img, sample_dir, "joined", 2, 0)
      display(Image.open(os.path.join(sample_dir, f"joined_{str(0).zfill(6)}.jpg")).resize((512, 256)))